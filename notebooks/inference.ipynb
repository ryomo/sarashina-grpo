{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from sarashina_grpo.config import PROJECT_ROOT\n",
    "\n",
    "# Add the `src` directory to the Python module search path\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, 'src'))\n",
    "\n",
    "###################\n",
    "# Config\n",
    "###################\n",
    "\n",
    "# fmt: off\n",
    "MODEL_NAME = \"sbintuitions/sarashina2.2-3b-instruct-v0.1\"  # https://huggingface.co/sbintuitions\n",
    "# MODEL_NAME = f\"{PROJECT_ROOT}/artifact/outputs/checkpoint-500\"  # Path to the checkpoint directory\n",
    "LORA_RANK = 32  # Larger rank = smarter, but slower\n",
    "\n",
    "# MAX_SEQ_LENGTH ≧ MAX_PROMPT_LENGTH + MAX_COMPLETION_LENGTH\n",
    "MAX_SEQ_LENGTH = 4096  # Can increase for longer reasoning traces\n",
    "MAX_PROMPT_LENGTH = 1024  # default 512 - Maximum length of the prompt. If the prompt is longer than this value, it will be truncated left.\n",
    "MAX_COMPLETION_LENGTH = 512  # default 256 -  Maximum length of the generated completion.\n",
    "\n",
    "USE_VLLM = True  # Enable vLLM for fast inference\n",
    "\n",
    "GRPO_OUTPUT_DIR = f\"{PROJECT_ROOT}/artifact/grpo\"  # Contains the checkpoints\n",
    "# fmt: on\n",
    "\n",
    "###################\n",
    "# Load the model\n",
    "###################\n",
    "\n",
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,  # False for LoRA 16bit\n",
    "    fast_inference=USE_VLLM,  # Enable vLLM fast inference\n",
    "    max_lora_rank=LORA_RANK,\n",
    "    gpu_memory_utilization=0.6,  # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer, set_seed\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n",
    "\n",
    "from sarashina_grpo.config import SYSTEM_PROMPT\n",
    "\n",
    "# # Check the types\n",
    "# print(type(model))      # => <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
    "# print(type(tokenizer))  # => <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
    "\n",
    "# Type hints\n",
    "model: LlamaForCausalLM\n",
    "tokenizer: LlamaTokenizerFast\n",
    "\n",
    "\n",
    "###################\n",
    "# Inference\n",
    "###################\n",
    "\n",
    "# https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-11.-inference-running-the-model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# set_seed(123)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "\n",
    "def remobe_s_tag(text: str, stream_end: bool = False):\n",
    "    \"\"\"Override the on_finalized_text method of TextStreamer to remove the </s> tag.\"\"\"\n",
    "    text = text.replace(\"</s>\", \"\")\n",
    "    print(text, flush=True, end=\"\" if not stream_end else None)\n",
    "\n",
    "\n",
    "streamer.on_finalized_text = remobe_s_tag\n",
    "\n",
    "\n",
    "def print_and_return_response(streamer, tokenizer, model, prompts):\n",
    "\n",
    "    print(f\"user: {prompts[-1]['content']}\")\n",
    "    print(\"ai:\")\n",
    "\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        formatted_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True,\n",
    "        # Use randomness when choosing words\n",
    "        do_sample=True,  # If False, the model always picks the most likely next word. Default False\n",
    "        temperature=1.0,  # Higher temperature = more randomness. Default 1.0\n",
    "        top_k=50,  # Limits the selection of next words. Default 50\n",
    "        top_p=1.0,  # top_p=1.0 means no limit; top_p=0.9 would restrict to the top words that make up 90% of the probability. Default 1.0\n",
    "    )\n",
    "\n",
    "    def extract_response(text: str) -> str:\n",
    "        \"\"\"Extract response from AI output.\"\"\"\n",
    "        text = text.split(\"<|assistant|>\")[-1]\n",
    "        text = text.split(\"</s>\")[0]\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    return extract_response(output)\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"今日は疲れたので休みたい。\"},\n",
    "]\n",
    "response = print_and_return_response(streamer, tokenizer, model, prompts)\n",
    "\n",
    "\n",
    "prompts.extend(\n",
    "    [\n",
    "        {\"role\": \"assistant\", \"content\": response},\n",
    "        {\"role\": \"user\", \"content\": \"あなたも同じですか？\"},\n",
    "    ]\n",
    ")\n",
    "response = print_and_return_response(streamer, tokenizer, model, prompts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
